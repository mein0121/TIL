# ëª¨ë¸ í‰ê°€
## ë¶„ë¥˜ì™€ íšŒê·€ì˜ í‰ê°€ë°©ë²•
- sklearn.metrics ëª¨ë“ˆì„ í†µí•´ ì œê³µ

### ë¶„ë¥˜ í‰ê°€ ì§€í‘œ
1. ì •í™•ë„ (Accuracy)
	- ì •í™•ë„(ğ´ğ‘ğ‘ğ‘¢ğ‘Ÿğ‘ğ‘ğ‘¦)=ë§ê²Œì˜ˆì¸¡í•œê±´ìˆ˜ / ì „ì²´ì˜ˆì¸¡ê±´ìˆ˜
1. ì •ë°€ë„ (Precision)
    - Positive(ì–‘ì„±)ìœ¼ë¡œ ì˜ˆì¸¡ í•œ ê²ƒ ì¤‘ ì‹¤ì œ Positive(ì–‘ì„±)ì¸ ë¹„ìœ¨
    - **PPV**(Positive Predictive Value) ë¼ê³ ë„ í•œë‹¤.
1. ì¬í˜„ë¥ /ë¯¼ê°ë„ (Recall/Sensitivity)
    - ì‹¤ì œ Positive(ì–‘ì„±)ì¸ ê²ƒ ì¤‘ì— Positive(ì–‘ì„±)ë¡œ ì˜ˆì¸¡ í•œ ê²ƒì˜ ë¹„ìœ¨
    - **TPR**(True Positive Rate) ì´ë¼ê³ ë„ í•œë‹¤.
1. F1ì ìˆ˜ (F1 Score)
    - ì •ë°€ë„ì™€ ì¬í˜„ìœ¨ì˜ ì¡°í™”í‰ê·  ì ìˆ˜
	- recallê³¼ precisionì´ ë¹„ìŠ·í•  ìˆ˜ë¡ ë†’ì€ ê°’ì„ ê°€ì§€ê²Œ ëœë‹¤
1. PR Curve, AP
1. ROC, AUC

#### ê¸°íƒ€
1. Specificity(íŠ¹ì´ë„)
    - ì‹¤ì œ Negative(ìŒì„±)ì¸ ê²ƒë“¤ ì¤‘ Negative(ìŒì„±)ìœ¼ë¡œ ë§ê²Œ ì˜ˆì¸¡ í•œ ê²ƒì˜ ë¹„ìœ¨
    - TNR(True Negative Rate) ë¼ê³ ë„ í•œë‹¤.
1. Fall out(ìœ„ì–‘ì„±ë¥ )
    - ì‹¤ì œ Negative(ìŒì„±)ì¸ ê²ƒë“¤ ì¤‘ Positive(ì–‘ì„±)ìœ¼ë¡œ ì˜ëª» ì˜ˆì¸¡í•œ ê²ƒì˜ ë¹„ìœ¨. `1 - íŠ¹ì´ë„`
    - **FPR** (False Positive Rate) ë¼ê³ ë„ í•œë‹¤.
	- ğ¹ğ‘ğ‘™ğ‘™âˆ’ğ‘‚ğ‘¢ğ‘¡(ğ¹ğ‘ƒğ¹) = ğ¹ğ‘ƒ / ğ‘‡ğ‘+ğ¹ğ‘ƒ
	
### íšŒê·€ í‰ê°€ë°©ë²•
1. MSE (Mean Squared Error)
1. RMSE (Root Mean Squared Error)
3. ğ‘…2(ê²°ì •ê³„ìˆ˜)

## í˜¼ë™ í–‰ë ¬(Confusion Marix)
- ë¶„ë¥˜ì˜ í‰ê°€ì§€í‘œì˜ ê¸°ì¤€ìœ¼ë¡œ ì‚¬ìš©ëœë‹¤.
- í˜¼ë™í–‰ë ¬ì„ ì´ìš©í•´ ë‹¤ì–‘í•œ í‰ê°€ì§€í‘œ(ì •í™•ë„, ì¬í˜„ë¥ , ì •ë°€ë„, F1 ì ìˆ˜, AUC ì ìˆ˜)ë¥¼ ê³„ì‚°í•  ìˆ˜ ìˆë‹¤.
- í•¨ìˆ˜: confusion_matrix(ì •ë‹µ, ëª¨ë¸ì˜ˆì¸¡ê°’)
- ê²°ê³¼ì˜ 0ë²ˆì¶•: ì‹¤ì œ(Ground Truth) class, 1ë²ˆ ì¶•: ì˜ˆì¸¡ class
![image](/images/predicted.png)
- TP(True Positive) - ì–‘ì„±ìœ¼ë¡œ ì˜ˆì¸¡í–ˆëŠ”ë° ë§ì€ ê°œìˆ˜
- TN(True Negative) - ìŒì„±ìœ¼ë¡œ ì˜ˆì¸¡í–ˆëŠ”ë° ë§ì€ ê°œìˆ˜
- FP(False Positive) - ì–‘ì„±ìœ¼ë¡œ ì˜ˆì¸¡í–ˆëŠ”ë° í‹€ë¦° ê°œìˆ˜ (ìŒì„±ì„ ì–‘ì„±ìœ¼ë¡œ ì˜ˆì¸¡)
- FN(False Negative) - ìŒì„±ìœ¼ë¡œ ì˜ˆì¸¡í–ˆëŠ”ë° í‹€ë¦° ê°œìˆ˜ (ì–‘ì„±ì„ ìŒì„±ìœ¼ë¡œ ì˜ˆì¸¡)

## ê° í‰ê°€ ì§€í‘œ ê³„ì‚° í•¨ìˆ˜
- sklearn.metrics ëª¨ë“ˆ
`from sklearn.metrics import confusion_matrix, plot_confusion_matrix, recall_score, precision_score, f1_score, accuracy_score`

- ### confusion_matrix(y ì‹¤ì œê°’, y ì˜ˆì¸¡ê°’)
    - í˜¼ëˆ í–‰ë ¬ ë°˜í™˜
- ### recall_score(y ì‹¤ì œê°’, y ì˜ˆì¸¡ê°’) 
  - Recall(ì¬í˜„ìœ¨) ì ìˆ˜ ë°˜í™˜ (Positive ì¤‘ Positiveë¡œ ì˜ˆì¸¡í•œ ë¹„ìœ¨ (TPR))
- ### precision_score(y ì‹¤ì œê°’, y ì˜ˆì¸¡ê°’)
  - Precision(ì •ë°€ë„) ì ìˆ˜ ë°˜í™˜ (Positiveë¡œ ì˜ˆì¸¡í•œ ê²ƒ ì¤‘ Positiveì¸ ê²ƒì˜ ë¹„ìœ¨ (PPV))
- ### f1_score(y ì‹¤ì œê°’, y ì˜ˆì¸¡ê°’)
    - F1 ì ìˆ˜ ë°˜í™˜ (recallê³¼ precisionì˜ ì¡°í™” í‰ê· ê°’)
- ### classification_report(y ì‹¤ì œê°’, y ì˜ˆì¸¡ê°’) 
    - í´ë˜ìŠ¤ ë³„ë¡œ recall, precision, f1 ì ìˆ˜ì™€ accuracyë¥¼ ì¢…í•©í•´ì„œ ë³´ì—¬ì¤€ë‹¤.
`from sklearn.metrics import classification_report`

## ì¬í˜„ìœ¨ê³¼ ì •ë°€ë„ì˜ ê´€ê³„
#### ì¬í˜„ìœ¨ì´ ë” ì¤‘ìš”í•œ ê²½ìš°
- ì‹¤ì œ Positive ë°ì´í„°ë¥¼ Negative ë¡œ ì˜ëª» íŒë‹¨í•˜ë©´ ì—…ë¬´ìƒ í° ì˜í–¥ì´ ìˆëŠ” ê²½ìš°. 
- FN(False Negative)ë¥¼ ë‚®ì¶”ëŠ”ë° ì´›ì ì„ ë§ì¶˜ë‹¤.
- ex) ì•”í™˜ì íŒì • ëª¨ë¸, ë³´í—˜ì‚¬ê¸° ì ë°œ ëª¨ë¸

#### ì •ë°€ë„ê°€ ë” ì¤‘ìš”í•œ ê²½ìš°
- ì‹¤ì œ Negative ë°ì´í„°ë¥¼ Positive ë¡œ ì˜ëª» íŒë‹¨í•˜ë©´ ì—…ë¬´ìƒ í° ì˜í–¥ì´ ìˆëŠ” ê²½ìš°.
- FP(False Positive)ë¥¼ ë‚®ì¶”ëŠ”ë° ì´ˆì ì„ ë§ì¶˜ë‹¤.
- ex) ìŠ¤íŒ¸ë©”ì¼ íŒì • ëª¨ë¸

## ì„ê³„ê°’(Threshold) ë³€ê²½ì„ í†µí•œ ì¬í˜„ìœ¨, ì •ë°€ë„ ë³€í™˜
- ì„ê³„ê°’ : ëª¨ë¸ì´ ë¶„ë¥˜ì˜ ë‹µì„ ê²°ì •í•  ë•Œ ê¸°ì¤€ê°’
- **ì„ê³„ê°’ì„ ë‚®ì¶”ë©´ ì¬í˜„ìœ¨ì€ ì˜¬ë¼ê°€ê³  ì •ë°€ë„ëŠ” ë‚®ì•„ì§„ë‹¤.**
- **ì„ê³„ê°’ì„ ë†’ì´ë©´ ì¬í˜„ìœ¨ì€ ë‚®ì•„ì§€ê³  ì •ë°€ë„ëŠ” ì˜¬ë¼ê°„ë‹¤.**
- ì„ê³„ê°’ì„ ë³€í™”ì‹œì¼°ì„ë•Œ **ì¬í˜„ìœ¨ê³¼ ì •ë°€ë„ëŠ” ìŒì˜ ìƒê´€ê´€ê³„ë¥¼ ê°€ì§„ë‹¤.**

### Binarizer - ì„ê³„ê°’ ë³€ê²½
- Transformerë¡œ ì–‘ì„± ì—¬ë¶€ë¥¼ ì„ íƒí•˜ëŠ” ì„ê³„ê°’ì„ ë³€ê²½í•  ìˆ˜ ìˆë‹¤.
```
from sklearn.preprocessing import Binarizer
# ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì— ì ìš©
pos_proba = tree.predict_proba(X_test) #positiveë“¤ì˜ í™•ë¥  
binarizer = Binarizer(threshold=0.5) # ì„ê³„ê°’ì„ ì§€ì •
binarizer.fit(pos_proba)
predict = binarizer.transform(pos_proba)[:, 1]
```

## PR Curve(Precision Recall Curve-ì •ë°€ë„ ì¬í˜„ìœ¨ ê³¡ì„ )ì™€ AP Score(Average Precision Score)
![image](/images/prcAndaps.png)
- ì„ê³„ê°’ì´ 0 â†’ 1 ë³€í™”í• ë•Œ ì¬í˜„ìœ¨(recall)ê³¼ ì •ë°€ë„(precision)ì˜ ë³€í™”ë¥¼ ì´ìš©í•œ í‰ê°€ ì§€í‘œ
- AP Score
    - PR Curveì˜ ì„ ì•„ë˜ ë©´ì ì„ ê³„ì‚°í•œ ê°’ìœ¼ë¡œ ë†’ì„ ìˆ˜ë¡ ì„±ëŠ¥ì´ ìš°ìˆ˜í•˜ë‹¤.

## ROC curve(Receiver Operating Characteristic Curve)ì™€ AUC(Area Under the Curve) score
![image](/images/rocAndauc.png)
- **FPR(False Positive Rate-ìœ„ì–‘ì„±ìœ¨)**
    - ìœ„ì–‘ì„±ìœ¨ (fall-out)
    - ì‹¤ì œ ìŒì„±ì¤‘ ì–‘ì„±ìœ¼ë¡œ ì˜ëª» ì˜ˆì¸¡ í•œ ë¹„ìœ¨
	- ğ¹ğ‘ƒ / ğ‘‡ğ‘+ğ¹ğ‘ƒ
- **TPR(True Positive Rate-ì¬í˜„ìœ¨/ë¯¼ê°ë„)** 
    - ì¬í˜„ìœ¨(recall)
    - ì‹¤ì œ ì–‘ì„±ì¤‘ ì–‘ì„±ìœ¼ë¡œ ë§ê²Œ ì˜ˆì¸¡í•œ ë¹„ìœ¨
	- ğ‘‡ğ‘ƒ / ğ¹ğ‘+ğ‘‡ğ‘ƒ
- **ROC ê³¡ì„ **
    - 2ì§„ ë¶„ë¥˜ì˜ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ì§€í‘œ ì¤‘ í•˜ë‚˜.
    - ë¶ˆê· í˜• ë°ì´í„°ì…‹ì„ í‰ê°€í•  ë•Œ ì‚¬ìš©.
- **AUC**
    - ROC ê³¡ì„  ì•„ë˜ìª½ ë©´ì 
    - 0 ~ 1 ì‚¬ì´ ì‹¤ìˆ˜ë¡œ ë‚˜ì˜¤ë©° í´ìˆ˜ë¡ ì¢‹ë‹¤.
```
from sklearn.metrics import precision_recall_curve, plot_precision_recall_curve, average_precision_score

pos_proba = tree.predict_proba(X_test)[:,1]
precisions, recalls, thresholds = precision_recall_curve(y_test, pos_proba) # y, positive_ì˜ˆì¸¡í™•ë¥ 

#ê·¸ë˜í”„ ê·¸ë¦¬ê¸°
ax = plt.gca() # Get Current Axes
plot_precision_recall_curve(tree, # model
                            X_train, # Xê°’
                            y_train, # yê°’
                            ax=ax)

# APS
average_precision_score(y_test, pos_proba)
```

### ROC, AUC ì ìˆ˜  í™•ì¸
- roc_curve(yê°’, ì˜ˆì¸¡í™•ë¥ ) : FPR, TPR, Thresholds (ì„ê³„ì¹˜)
- roc_auc_score(yê°’, ì˜ˆì¸¡í™•ë¥ ) : AUC ì ìˆ˜ ë°˜í™˜
```
from sklearn.metrics import roc_curve, plot_roc_curve, roc_auc_score

pos_proba_tree = tree.predict_proba(X_test)[:,1]
fpr_tree, tpr_tree, threshold_tree = roc_curve(y_test, pos_proba_tree)  # y, pos_ì˜ˆì¸¡í™•ë¥ 

#ê·¸ë˜í”„ ê·¸ë¦¬ê¸°.
ax = plt.gca() # Get Current Axes
plot_roc_curve(tree, # model
			   X_test, # Xê°’
			   y_test, # yê°’
			   ax=ax)
#plt.plot(fpr_tree, tpr_tree)

#roc_auc_score í™•ì¸.
roc_auc_score(y_test, pos_proba_tree)
```








