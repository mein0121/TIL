# Scikit_learn
## ì‚¬ì´í‚·ëŸ° ì£¼ìš”ëª¨ë“ˆ.
![image](/images/scikit_learn.png)

### scikit_learn ê°œë°œ íŒ¨í„´
	1. ë°ì´í„° ë¶„í• 
		- training, validation, test ì…‹ìœ¼ë¡œ ë¶„ë¦¬.
	2. ëª¨ë¸ ìƒì„±
		- ì˜ˆì¸¡ ëª©ì ì— ë§ëŠ” ëª¨ë¸ìƒì„±
		- í•˜ì´í¼ íŒŒë¼ë¯¸í„° ì„¤ì •.
	3. ëª¨ë¸ í•™ìŠµ
		- fit, í›ˆë ¨ë°ì´í„°ë¡œ ëª¨ë¸ í•™ìŠµ ë˜ëŠ” íŠ¹ì§• ì¶”ì¶œ
	4. ì˜ˆì¸¡
		- predict / predict_prob (ì˜ˆì¸¡), transform(ë³€í™˜)
	5. í‰ê°€
		- ëª¨ë¸ ì„±ëŠ¥ í‰ê°€, ì •í™•ë„,R2,MSE ë“±, ì ì ˆí•œ í‰ê°€í•¨ìˆ˜ ì´ìš© ê²°ê³¼ í™•ì¸.

### Scikit_learn ë°ì´í„°ì…‹ 
- sklearn.datasets.load_xxxx ì´ìš©
- êµ¬ì„±
    - **target_names**: ì˜ˆì¸¡í•˜ë ¤ëŠ” ê°’(class)ì„ ê°€ì§„ ë¬¸ìì—´ ë°°ì—´
    - **target**: Label(ì¶œë ¥ë°ì´í„°)
    - **data**: Feature(ì…ë ¥ë³€ìˆ˜)
    - **feature_names**: ì…ë ¥ë³€ìˆ˜ ê° í•­ëª©ì˜ ì´ë¦„
    - **DESCR**: ë°ì´í„°ì…‹ì— ëŒ€í•œ ì„¤ëª…

### ë°ì´í„°ì…‹ ë¶„í• 
- scikit-learnì˜ train_test_split() í•¨ìˆ˜ ì´ìš©
#### Holdout
- ë°ì´í„°ì…‹ì„ Train set, Validation set, Test setìœ¼ë¡œ ë‚˜ëˆˆë‹¤.
- ë‹¨ì 
	- train/test ì…‹ì´ ì–´ë–»ê²Œ ë‚˜ëˆ  ì§€ëƒì— ë”°ë¼ ê²°ê³¼ê°€ ë‹¬ë¼ì§„ë‹¤.
	- ë°ì´í…Œì…‹ì˜ ì–‘ì´ ì ì„ ê²½ìš° í•™ìŠµì„ ìœ„í•œ ë°ì´í„° ì–‘ì´ ë„ˆë¬´ ì ì–´ í•™ìŠµì´ ì œëŒ€ë¡œ ì•ˆë  ìˆ˜ ìˆë‹¤.
```
X_train, X_test, Y_train, Y_test = train_test_split(df['data'],   #input dataset
												    df['target'], #ouput dataset
												    test_size=0.2,  # test setì˜ ë¹„ìœ¨(0 ~ 1), default: 0.25
												    stratify=df['target'], # ê° í´ë˜ìŠ¤(ë¶„ë¥˜ëŒ€ìƒ)ë“¤ì„ ì›ë³¸ë°ì´í„°ì…‹ê³¼ ê°™ì€ ë¹„ìœ¨ë¡œ ë‚˜ëˆ ë¼.
												    random_state=1) # randomì˜ seedê°’ ì •ì˜
```

#### K-fold cross validation
- ë°ì´í„°ì…‹ì„ K ê°œë¡œ ë‚˜ëˆˆ ë’¤ í•˜ë‚˜ë¥¼ ê²€ì¦ì„¸íŠ¸ë¡œ ë‚˜ë¨¸ì§€ë¥¼ í›ˆë ¨ì„¸íŠ¸ë¡œ í•˜ì—¬ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤ê³  í‰ê°€í•œë‹¤. 
- ë‚˜ë‰œ Kê°œì˜ ë°ì´í„°ì…‹ì´ í•œë²ˆì”© ê²€ì¦ì„¸íŠ¸ê°€ ë˜ë„ë¡ Kë²ˆ ë°˜ë³µí•˜ì—¬ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¨ ë’¤ ë‚˜ì˜¨ í‰ê°€ì§€í‘œë“¤ì„ í‰ê· ë‚´ì„œ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•œë‹¤.
- ì¢…ë¥˜
    - K-Fold
    - Stratified K-Fold
```
from sklearn.model_selection import KFold
# ê°ì²´ë¥¼ ìƒì„±í•˜ë©´ì„œ ëª‡ê°œì˜ foldë¡œ ë‚˜ëˆŒì§€ (Kê°’)ì„ ì§€ì •. 
kfold = KFold(n_splits=5)

# kfold.split(ë‚˜ëˆ„ë ¤ëŠ” InputData): Generatorë°˜í™˜. train/test setì˜ indexë¥¼ ë°˜í™˜
for train_index, test_index  in kfold.split(X):
    # ë°ì´í„°ì…‹ ë¶„ë¦¬
    X_train, y_train = X[train_index], y[train_index]
    X_test, y_test = X[test_index], y[test_index]
```
- ë‹¨ì 
	- ì› ë°ì´í„°ì…‹ì˜ row ìˆœì„œëŒ€ë¡œ ë¶„í• í•˜ê¸° ë•Œë¬¸ì— ë¶ˆê· í˜• ë¬¸ì œê°€ ë°œìƒí•  ìˆ˜ ìˆë‹¤.

#### Stratified K í´ë“œ
- ë‚˜ë‰œ fold ë“¤ì— labelë“¤ì´ ê°™ì€(ë˜ëŠ” ê±°ì˜ ê°™ì€) ë¹„ìœ¨ë¡œ êµ¬ì„± ë˜ë„ë¡ ë‚˜ëˆˆë‹¤. 
- from sklearn.model_selection.StratifiedKFold ì´ìš©
```
from sklearn.model_selection import StratifiedKFold
s_fold = StratifiedKFold(n_splits=3) #ê°ì²´ ìƒì„±ì‹œ ëª‡ê°œì˜ foldë¡œ ë‚˜ëˆŒì§€ ì§€ì • (Kê°’)

# s_fold.split(ë‚˜ëˆ„ë ¤ëŠ” InputData): Generatorë°˜í™˜. train/test setì˜ indexë¥¼ ë°˜í™˜
for train_index, test_index in s_fold.split(X, y):
    # train, test set data ë¶„ë¦¬/ìƒì„±
    X_train, y_train = X[train_index], y[train_index]
    X_test, y_test = X[test_index], y[test_index]
```

### Decision Tree ëª¨ë¸ì„±.
1. import model
`from sklearn.tree import DecisionTreeClassifier`
2. ëª¨ë¸ ìƒì„±
`tree = DecisionTreeClassifier()`
3. ëª¨ë¸ í•™ìŠµ
`tree.fit(X_train, y_train']) # input_data(feature), output_data(label)`
4. ì˜ˆì¸¡
`tree.predict(X_train)`

### í‰ê°€
- sklearn.metrics ì´ìš©
`from sklearn.metrics import accuracy_score #ì •í™•ë„ ê²€ì¦í•˜ëŠ” í•¨ìˆ˜`

#### cross_val_score()
- ë°ì´í„°ì…‹ì„ Kê°œë¡œ ë‚˜ëˆ„ê³  Kë²ˆ ë°˜ë³µí•˜ë©´ì„œ í‰ê°€í•˜ëŠ” ì‘ì—…ì„ ì²˜ë¦¬í•´ ì£¼ëŠ” í•¨ìˆ˜
- ì£¼ìš”ë§¤ê°œë³€ìˆ˜
    - estimator: í•™ìŠµí•  í‰ê°€ëª¨ë¸ê°ì²´
    - X: feature
    - y: label
    - scoring: í‰ê°€ì§€í‘œ
    - cv: ë‚˜ëˆŒ ê°œìˆ˜ (K)
- ë°˜í™˜ê°’: array - ê° ë°˜ë³µë§ˆë‹¤ì˜ í‰ê°€ì ìˆ˜   
```
from sklearn.model_selection import cross_val_score
scores = cross_val_score(estimator=tree, # ëª¨ë¸ ì§€ì •.
                         X=X, # feature
                         y=y, # label
                         scoring='accuracy',
                         cv=3)
```

## Data preprocessing		
### ë ˆì´ë¸” ì¸ì½”ë”©(Label encoding)
- ë¬¸ìì—´(ë²”ì£¼í˜•) ê°’ì„ ì˜¤ë¦„ì°¨ìˆœ ì •ë ¬ í›„ 0 ë¶€í„° 1ì”© ì¦ê°€í•˜ëŠ” ê°’ìœ¼ë¡œ ë³€í™˜
- sklearn.preprocessing.LabelEncoder ì´ìš©
    - inverse_transform():ìˆ«ìë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
    - classes_ : ì¸ì½”ë”©í•œ í´ë˜ìŠ¤ ì¡°íšŒ

### One-Hot encoding
- Nê°œì˜ í´ë˜ìŠ¤ë¥¼ N ì°¨ì›ì˜ One-Hot ë²¡í„°ë¡œ í‘œí˜„ë˜ë„ë¡ ë³€í™˜
    - ê³ ìœ ê°’ë“¤ì„ í”¼ì²˜ë¡œ ë§Œë“¤ê³  ì •ë‹µì— í•´ë‹¹í•˜ëŠ” ì—´ì€ 1ë¡œ ë‚˜ë¨¸ì§„ 0ìœ¼ë¡œ í‘œì‹œí•œë‹¤.
- í•™ìŠµì‹œí‚¬ê²½ìš° 2ì°¨ì›ë°°ì—´ë¡œ ë³€í™˜í•˜ì—¬ ì ìš©
- sklearn.preprocessing.OneHotEncoder ì´ìš©
`onehotencoding.fit(items.reshape(-1,1))`
`ohe2.fit_transform(items[..., np.newaxis])`
`pd.get_dummies(df, columns=['column1', 'column2']) # columnsì— ì¸ì½”ë”© ëŒ€ìƒ ì»¬ëŸ¼ë“¤ì„ ì§€ì •í•œë‹¤.`

### StandardScaler (í‘œì¤€í™”)
- í”¼ì³ì˜ ê°’ë“¤ì´ í‰ê· ì´ 0ì´ê³  í‘œì¤€í¸ì°¨ê°€ 1ì¸ ë²”ìœ„(í‘œì¤€ì •ê·œë¶„í¬)ì— ìˆë„ë¡ ë³€í™˜í•œë‹¤.
    - 0ì„ ê¸°ì¤€ìœ¼ë¡œ ëª¨ë“  ë°ì´í„°ë“¤ì´ ëª¨ì—¬ìˆê²Œ ëœë‹¤
- sklearn.preprocessing.StandardScaler ì´ìš©

### MinMaxScaler
- ë°ì´í„°ì…‹ì˜ ëª¨ë“  ê°’ì„ 0ê³¼ 1 ì‚¬ì´ì˜ ê°’ìœ¼ë¡œ ë³€í™˜í•œë‹¤.
- ğ‘ğ‘’ğ‘¤_ğ‘¥ğ‘– = ğ‘¥ğ‘–âˆ’ğ‘šğ‘–ğ‘›(ğ‘‹) / ğ‘šğ‘ğ‘¥(ğ‘‹)âˆ’ğ‘šğ‘–ğ‘›(ğ‘‹)
- sklearn.preprocessing.MinMaxScaler ì´ìš©

